{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea44cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Loading the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Defining labels: 0 for non-virginica, 1 for virginica\n",
    "y_binary = (y == 2).astype(int)\n",
    "#In this above code, original labels (0,1,2) are converted to binary format.\n",
    "#it checls if the label is equal to 2 because this label corresponds to virginica in this dataset and it converts it to 1.\n",
    "\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initializing the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4f5f6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#Failure modes: in which data instances is the model wrong? (1 point)\n",
    "\n",
    "\n",
    "#Identifying instances in the test set that were misclassfied by the logistic regression model.\n",
    "misclassified_indices = (y_pred != y_test)\n",
    "misclassified_instances = X_test[misclassified_indices]\n",
    "\n",
    "print(misclassified_instances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4c4e6a",
   "metadata": {},
   "source": [
    "In this case, the model did not make any mistakes on the test set. There are no instances that were classified incorrectly (i.e., all predictions matched the true labels). Therefore, there are no \"failure modes\" in this specific evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d45dc4a",
   "metadata": {},
   "source": [
    "#Are there any shared properties for these cases? (1 point)\n",
    "\n",
    "In this case, we can say that there are no misclassified instances, and therefore, there are no shared properties to analyze among misclassified cases. The model performed perfectly on this particular test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98ac99e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      "[[19  0]\n",
      " [ 0 11]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How is the model doing across a set of evaluation metrics: accuracy and confusion metric. (1 point)\n",
    "\n",
    "# Calculating Evaluation metrics like accuracy and confusion matrix.\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_mat}\")\n",
    "print(f\"Classification Report:\\n{report}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca7c3c",
   "metadata": {},
   "source": [
    "From the above measurements, it can be said that Accuracy is 1.0 which implies that the model correctly classified all of the instances in the test set.\n",
    "\n",
    "If we have to saye more about confusion matrix then it can be said that the first row represents the instances with true label 0 which is non-verginica. 0 instances were incorrectly classified as verginica (false positive). 19 instances were correctly explained as non-verginica (true negatives).\n",
    "\n",
    "The second row represents the instances with true label 1 (virginica). 11 instances were correctly classified as virginica (true positives). 0 instances were incorrectly classified as non-verginica (false negatives).\n",
    "\n",
    "We can also calculate precision, recall, f1-score and support as well as accuracy, macro average and weighted avergage using classification report function. \n",
    "\n",
    "Precision: It is the ratio of true positives to the sum of true positives and false positives. It measures the accuracy of the positive predictions. In this case, it's 1.0 for both classes, meaning all positive predictions were correct.\n",
    "\n",
    "Recall: It is the ratio of true positives to the sum of true positives and false negatives. It measures the ability of the model to correctly identify all instances of the positive class. Again, it's 1.0 for both classes which are non-verginica and verginica.\n",
    "\n",
    "F1-score: It is the harmonic mean of precision and recall. It provides a balance between precision and recall. A high F1-score indicates good performance.\n",
    "\n",
    "Support: It is the number of occurrences of each class in the test set.\n",
    "\n",
    "The 'accuracy', 'macro avg', and 'weighted avg' are aggregate measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b725a24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
